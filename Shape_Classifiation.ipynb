{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Symbolics\n",
    "using SphericalHarmonics\n",
    "using CUDA\n",
    "using NNlib\n",
    "\n",
    "using TensorCast\n",
    "\n",
    "# Catching slow GPU errors\n",
    "CUDA.allowscalar(false)\n",
    "\n",
    "using BenchmarkTools\n",
    "using ProgressMeter\n",
    "\n",
    "include(\"utils.jl\")\n",
    "include(\"Spherical.jl\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×3 Matrix{Int64}:\n",
       " 0  0  0\n",
       " 0  0  1\n",
       " 1  0  0\n",
       " 1  1  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Tetris Shapes\n",
    "tetris = [[0 0 0; 0 0 1; 1 0 0; 1 1 0],  # chiral_shape_1\n",
    "          [0 0 0; 0 0 1; 1 0 0; 1 -1 0], # chiral_shape_2\n",
    "          [0 0 0; 1 0 0; 0 1 0; 1 1 0],  # square\n",
    "          [0 0 0; 0 0 1; 0 0 2; 0 0 3],  # line\n",
    "          [0 0 0; 0 0 1; 0 1 0; 1 0 0],  # corner\n",
    "          [0 0 0; 0 0 1; 0 0 2; 0 1 0],  # T\n",
    "          [0 0 0; 0 0 1; 0 0 2; 0 1 1],  # zigzag\n",
    "          [0 0 0; 1 0 0; 1 1 0; 2 1 0]]  # L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One simple ℝ ≥ 0 -> ℝ function broadcasted across every elements of the array.\n",
    "Function is a linear combination of basis functions ∑ᵢ aᵢ rbfᵢ(r), with learned weightings aᵢ.\n",
    "\"\"\"\n",
    "struct RLayer\n",
    "    as::Vector{Float32}\n",
    "    # TODO Maybe add another NN to copy TFN\n",
    "    \n",
    "    centers::Vector{Float32}\n",
    "    γ::Float32\n",
    "end\n",
    "\n",
    "function RLayer(centers; init=Flux.glorot_uniform)\n",
    "    n_basis = length(centers)\n",
    "    as = init(n_basis)\n",
    "\n",
    "    γ = (centers[end] - centers[1]) / n_basis\n",
    "    RLayer(as, centers, γ)\n",
    "end\n",
    "\n",
    "function (R::RLayer)(radials)\n",
    "    reduce(+, [a * @.(exp(- R.γ * (radials - c)^2)) for (a, c) in zip(R.as, R.centers)])\n",
    "end\n",
    "\n",
    "Flux.@functor RLayer (as,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct FLayer\n",
    "    #R::Chain # Radial NN\n",
    "    R::RLayer\n",
    "\n",
    "    Ys::Vector{Function} # SH functions for this ℓf\n",
    "    ℓf::Int # Filter angular momentum # TODO Consider removing\n",
    "end\n",
    "\n",
    "function FLayer(Ys::Vector{Function}, centers::Vector{Float32})\n",
    "    # Will later allow for custom spec\n",
    "    R = RLayer(centers)\n",
    "\n",
    "    ℓf = (length(Ys) - 1) ÷ 2\n",
    "    FLayer(R, Ys, ℓf)\n",
    "end\n",
    "\n",
    "# Dimension needs to be made one bigger\n",
    "function (F::FLayer)(rr)\n",
    "    # Apply R to the input radii\n",
    "    rr_radials = rr[:, :, 1, :] # TODO Possibly change this back to view\n",
    "\n",
    "    R_out = F.R(rr_radials)\n",
    "\n",
    "    # Multiply by SH components\n",
    "    θs = @view rr[:,:,2,:]\n",
    "    ϕs = @view rr[:,:,3,:]\n",
    "    Y_out = Flux.batch([Y.(θs, ϕs) for Y in F.Ys])\n",
    "    \n",
    "    R_out .* Y_out\n",
    "end\n",
    "\n",
    "Flux.@functor FLayer (R,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct CLayer\n",
    "    F::FLayer # Trainable NN\n",
    "    CG_mats::Dict{Tuple{Int, Int}, CuArray{Float32}} # Dictionary of matrices, keyed by (ℓo, mo)\n",
    "\n",
    "    ℓi::Int # Input ℓ\n",
    "    ℓf::Int # Filter ℓ\n",
    "    ℓms::Vector{Tuple{Int, Int}} # Specifying output order\n",
    "end\n",
    "\n",
    "# Constructor\n",
    "function CLayer(ℓi::Int, ℓf::Int, ℓos::Vector{Int}, centers::Vector{Float32})\n",
    "    @assert ℓos ⊆ abs(ℓi - ℓf):(ℓi + ℓf) \"Output `ℓo` not compatible with filter `ℓf` and input `ℓi`.\"\n",
    "\n",
    "    Ys = generate_Yℓms(ℓf)\n",
    "    F_NN = FLayer(Ys, centers)\n",
    "\n",
    "    # Not going to choose every one\n",
    "    ℓms::Vector{Tuple{Int, Int}} = []\n",
    "    CG_mats::Dict{Tuple{Int, Int}, CuArray{Float32}} = Dict()\n",
    "    for ℓo in ℓos\n",
    "        for mo in -ℓo:ℓo\n",
    "            push!(ℓms, (ℓo, mo))\n",
    "            CG_mat = zeros(Float32, (2ℓi + 1, 2ℓf + 1))\n",
    "            for (i_i, mi) in enumerate(-ℓi:ℓi)\n",
    "                for (i_f, mf) in enumerate(-ℓf:ℓf)\n",
    "                    # TODO Check that ordering of f and i is correct\n",
    "                    # Currently giving zero\n",
    "                    CG_mat[i_i, i_f] = cg(ℓi, mi, ℓf, mf, ℓo, mo)\n",
    "                end\n",
    "            end\n",
    "            CG_mats[(ℓo, mo)] = cu(CG_mat)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    CLayer(F_NN, CG_mats, ℓi, ℓf, ℓms)\n",
    "end\n",
    "\n",
    "# Forward pass\n",
    "\"\"\"\n",
    "V indexed by [mi, b]\n",
    "\"\"\"\n",
    "\n",
    "#=\n",
    "function (C::CLayer)(rr, V)\n",
    "    n_points, _, _, n_samples = size(rr)\n",
    "\n",
    "    F_out = permutedims(C.F(rr), (1, 4, 2, 3))\n",
    "    F_reshape = reshape(F_out, n_points, 2C.ℓf + 1, :)\n",
    "\n",
    "    V_repeat = repeat(V, inner=(1, 1, n_points))\n",
    "    F_tilde = batched_mul(V_repeat, F_reshape)\n",
    "\n",
    "    # TODO Make this general\n",
    "    # For now just assume one CG_mat\n",
    "    CG_mat = CuArray(C.CG_mats[C.ℓms[1]])\n",
    "\n",
    "    L_tilde = CG_mat .* F_tilde\n",
    "\n",
    "    # Steps using a lot of memory atm\n",
    "    L = sum(L_tilde, dims=(1, 2))#[1,1,:]\n",
    "    reshape(L, n_points, n_samples)\n",
    "end\n",
    "=#\n",
    "\n",
    "\n",
    "# Trying to use tensorcast\n",
    "function (C::CLayer)(rr, V)\n",
    "    F_out = C.F(rr)\n",
    "    \n",
    "    # Using Einstein summation convention for brevity\n",
    "    # Speed seems comparable\n",
    "    @reduce F_tilde[mi, mf, a, γ] := sum(b) V[mi, b, γ] * F_out[b, a, γ, mf]\n",
    "\n",
    "    # TODO Make this general\n",
    "    # For now just assume one CG_mat\n",
    "    CG_mat = C.CG_mats[C.ℓms[1]]\n",
    "\n",
    "    L_tilde = CG_mat .* F_tilde\n",
    "\n",
    "    @reduce L[a, γ] := sum(mi, mf) L_tilde[mi, mf, a, γ]\n",
    "end\n",
    "\n",
    "\n",
    "Flux.@functor CLayer (F,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×10000 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       "  0.0586906   0.148469    0.23939   …  -0.0931654   0.119467   0.259697\n",
       " -0.12375    -0.694081    0.318815      0.172496   -0.12617   -0.245736\n",
       " -0.154179    0.510324   -0.722801      0.0979667   0.302776  -0.532552\n",
       "  0.219239    0.0352885   0.164596     -0.177297   -0.296073   0.518592"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ℓi, ℓf, ℓos = 0, 1, [1]\n",
    "\n",
    "centers = range(0f0, 3.5f0; length=4) |> collect\n",
    "\n",
    "c_test = CLayer(ℓi, ℓf, ℓos, centers) |> gpu\n",
    "\n",
    "xs = rand(Float32, (4, 3, 100))\n",
    "xss = pairwise_rs(xs)\n",
    "rss = cart_to_sph(xss) |> gpu\n",
    "\n",
    "V = ones(Float32, (1, 4, 100)) |> gpu\n",
    "#V = ones(Float32, (1, 4))\n",
    "\n",
    "C_out = c_test(rss, V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Increasing Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FLayer(RLayer(Float32[0.11861226, -0.5310653, 0.11397445, 0.89927113], Float32[0.0, 1.1666666, 2.3333333, 3.5], 0.875f0), Function[var\"#24#25\"(), var\"#26#27\"(), var\"#28#29\"(), var\"#30#31\"(), var\"#32#33\"()], 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = rand(Float32, (4, 3, 6))\n",
    "xss = pairwise_rs(xs)\n",
    "rss = cart_to_sph(xss)\n",
    "\n",
    "sh = size(rss)\n",
    "ℓ = 2\n",
    "Ys = generate_Yℓms(ℓ)\n",
    "yss = rand(Float32, (sh[1], sh[2], sh[4], 2ℓ+1))\n",
    "\n",
    "rss_gpu = rss |> gpu\n",
    "yss_gpu = yss |> gpu\n",
    "\n",
    "centers = range(0f0, 3.5f0; length=4) |> collect\n",
    "f_test = FLayer(Ys, centers) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: \u001b[0mCannot `convert` an object of type \u001b[92mBool\u001b[39m\u001b[0m to an object of type \u001b[91mVector{Float32}\u001b[39m\n\u001b[0mClosest candidates are:\n\u001b[0m  convert(::Type{T}, \u001b[91m::Union{Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}} where T, Union{Base.LogicalIndex{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, Base.ReinterpretArray{T, N, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s14\"}, var\"#s14\"}} where var\"#s14\"<:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}, Base.ReshapedArray{T, N, <:Union{Base.ReinterpretArray{<:Any, <:Any, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s15\"}, var\"#s15\"}}, SubArray{<:Any, <:Any, var\"#s15\"}, var\"#s15\"}} where var\"#s15\"<:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}, SubArray{T, N, <:Union{Base.ReinterpretArray{<:Any, <:Any, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s16\"}, var\"#s16\"}}, Base.ReshapedArray{<:Any, <:Any, <:Union{Base.ReinterpretArray{<:Any, <:Any, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s16\"}, var\"#s16\"}}, SubArray{<:Any, <:Any, var\"#s16\"}, var\"#s16\"}}, var\"#s16\"}} where var\"#s16\"<:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}, LinearAlgebra.Adjoint{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Diagonal{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.LowerTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Symmetric{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Transpose{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Tridiagonal{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.UnitLowerTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.UnitUpperTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.UpperTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, PermutedDimsArray{T, N, <:Any, <:Any, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}} where {T, N}}\u001b[39m) where T<:Array at C:\\Users\\xboxe\\.julia\\packages\\NNlibCUDA\\kCpTE\\src\\batchedadjtrans.jl:15\n\u001b[0m  convert(::Type{Vector{T}}, \u001b[91m::AbstractAlgebra.Perm{T}\u001b[39m) where T at C:\\Users\\xboxe\\.julia\\packages\\AbstractAlgebra\\Oe2Uj\\src\\generic\\PermGroups.jl:49\n\u001b[0m  convert(::Type{Array{T, N}}, \u001b[91m::StaticArraysCore.SizedArray{S, T, N, N, Array{T, N}}\u001b[39m) where {S, T, N} at C:\\Users\\xboxe\\.julia\\packages\\StaticArrays\\jA1zK\\src\\SizedArray.jl:88\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: \u001b[0mCannot `convert` an object of type \u001b[92mBool\u001b[39m\u001b[0m to an object of type \u001b[91mVector{Float32}\u001b[39m\n\u001b[0mClosest candidates are:\n\u001b[0m  convert(::Type{T}, \u001b[91m::Union{Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}} where T, Union{Base.LogicalIndex{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, Base.ReinterpretArray{T, N, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s14\"}, var\"#s14\"}} where var\"#s14\"<:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}, Base.ReshapedArray{T, N, <:Union{Base.ReinterpretArray{<:Any, <:Any, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s15\"}, var\"#s15\"}}, SubArray{<:Any, <:Any, var\"#s15\"}, var\"#s15\"}} where var\"#s15\"<:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}, SubArray{T, N, <:Union{Base.ReinterpretArray{<:Any, <:Any, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s16\"}, var\"#s16\"}}, Base.ReshapedArray{<:Any, <:Any, <:Union{Base.ReinterpretArray{<:Any, <:Any, <:Any, <:Union{SubArray{<:Any, <:Any, var\"#s16\"}, var\"#s16\"}}, SubArray{<:Any, <:Any, var\"#s16\"}, var\"#s16\"}}, var\"#s16\"}} where var\"#s16\"<:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}, LinearAlgebra.Adjoint{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Diagonal{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.LowerTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Symmetric{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Transpose{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.Tridiagonal{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.UnitLowerTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.UnitUpperTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, LinearAlgebra.UpperTriangular{T, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}, PermutedDimsArray{T, N, <:Any, <:Any, <:Union{NNlib.BatchedAdjoint{T, <:CuArray{T}}, NNlib.BatchedTranspose{T, <:CuArray{T}}}}} where {T, N}}\u001b[39m) where T<:Array at C:\\Users\\xboxe\\.julia\\packages\\NNlibCUDA\\kCpTE\\src\\batchedadjtrans.jl:15\n\u001b[0m  convert(::Type{Vector{T}}, \u001b[91m::AbstractAlgebra.Perm{T}\u001b[39m) where T at C:\\Users\\xboxe\\.julia\\packages\\AbstractAlgebra\\Oe2Uj\\src\\generic\\PermGroups.jl:49\n\u001b[0m  convert(::Type{Array{T, N}}, \u001b[91m::StaticArraysCore.SizedArray{S, T, N, N, Array{T, N}}\u001b[39m) where {S, T, N} at C:\\Users\\xboxe\\.julia\\packages\\StaticArrays\\jA1zK\\src\\SizedArray.jl:88\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] RLayer(as::Bool, centers::Vector{Float32}, γ::Float32)",
      "    @ Main .\\In[2]:6",
      "  [2] (::var\"#10#11\"{RLayer})(y::NamedTuple{(:as,), Tuple{Bool}})",
      "    @ Main C:\\Users\\xboxe\\.julia\\packages\\Functors\\V2McK\\src\\functor.jl:19",
      "  [3] _default_walk(f::Function, x::RLayer)",
      "    @ Functors C:\\Users\\xboxe\\.julia\\packages\\Functors\\V2McK\\src\\functor.jl:39",
      "  [4] fmap(f::Flux.Train.var\"#1#2\", x::RLayer; exclude::typeof(Optimisers.isnumeric), walk::typeof(Functors._default_walk), cache::IdDict{Any, Any}, prune::Functors.NoKeyword)",
      "    @ Functors C:\\Users\\xboxe\\.julia\\packages\\Functors\\V2McK\\src\\functor.jl:46",
      "  [5] (::Functors.var\"#16#17\"{typeof(Optimisers.isnumeric), typeof(Functors._default_walk), IdDict{Any, Any}, Functors.NoKeyword, Flux.Train.var\"#1#2\"})(x::RLayer)",
      "    @ Functors C:\\Users\\xboxe\\.julia\\packages\\Functors\\V2McK\\src\\functor.jl:46",
      "  [6] map",
      "    @ .\\tuple.jl:221 [inlined]",
      "  [7] map(::Function, ::NamedTuple{(:R,), Tuple{RLayer}})",
      "    @ Base .\\namedtuple.jl:219",
      "  [8] _default_walk(f::Function, x::FLayer)",
      "    @ Functors C:\\Users\\xboxe\\.julia\\packages\\Functors\\V2McK\\src\\functor.jl:39",
      "  [9] fmap(f::Flux.Train.var\"#1#2\", x::FLayer; exclude::typeof(Optimisers.isnumeric), walk::typeof(Functors._default_walk), cache::IdDict{Any, Any}, prune::Functors.NoKeyword)",
      "    @ Functors C:\\Users\\xboxe\\.julia\\packages\\Functors\\V2McK\\src\\functor.jl:46",
      " [10] setup(rule::Optimisers.Adam{Float64}, model::FLayer)",
      "    @ Flux.Train C:\\Users\\xboxe\\.julia\\packages\\Flux\\kq9Et\\src\\train.jl:51",
      " [11] setup(rule::Adam, model::FLayer)",
      "    @ Flux C:\\Users\\xboxe\\.julia\\packages\\Flux\\kq9Et\\src\\deprecations.jl:117",
      " [12] top-level scope",
      "    @ In[11]:1"
     ]
    }
   ],
   "source": [
    "optim = Flux.setup(Flux.Adam(0.01), f_test)\n",
    "\n",
    "# Testing gradient\n",
    "losses = []\n",
    "@showprogress for epoch in 1:400\n",
    "    loss, grads = Flux.withgradient(f_test) do f\n",
    "        # Evaluate model and loss inside gradient context:\n",
    "        y_hat = f(rss_gpu)\n",
    "        Flux.mse(y_hat, yss_gpu)\n",
    "    end\n",
    "    Flux.update!(optim, f_test, grads[1])\n",
    "    push!(losses, loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Matrix{Float64}:\n",
       " 0.0595705  0.119141  0.178712"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = Chain(\n",
    "        Dense(1 => 5, relu),\n",
    "        Dense(5 => 1, relu)\n",
    "    )\n",
    "\n",
    "vector = [0.1, 0.2, 0.3]\n",
    "R(reshape(vector, (1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "@variables θ::Real, ϕ::Real\n",
    "Ys_sym = computeYlm(θ, ϕ; lmax=2, SHType=SphericalHarmonics.RealHarmonics())\n",
    "\n",
    "# Create dictionary of functions\n",
    "Ys = Dict(key => (eval ∘ Base.remove_linenums! ∘ build_function)(Ys_sym[key] |> simplify, θ, ϕ)\n",
    "            for key in Ys_sym.modes[1] |> collect)\n",
    "\n",
    "\n",
    "Ys_bodged = Dict(key => convert_expr_to_F32(build_function(Ys_sym[key] |> simplify, θ, ϕ))\n",
    "            for key in Ys_sym.modes[1] |> collect)\n",
    "\n",
    "ℓ = 2\n",
    "Ys_ℓ = [Ys_bodged[(ℓ, m)] for m in -ℓ:ℓ];"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
