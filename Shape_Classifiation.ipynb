{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Symbolics\n",
    "using SphericalHarmonics\n",
    "using CUDA\n",
    "using NNlib\n",
    "using OneHotArrays\n",
    "\n",
    "using TensorCast\n",
    "\n",
    "# Disable slow GPU indexing\n",
    "CUDA.allowscalar(false)\n",
    "\n",
    "using BenchmarkTools\n",
    "using ProgressMeter\n",
    "\n",
    "include(\"utils.jl\")\n",
    "include(\"Spherical.jl\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Tetris Shapes\n",
    "tetris = [[0 0 0; 0 0 1; 1 0 0; 1 1 0],  # chiral_shape_1\n",
    "          [0 0 0; 0 0 1; 1 0 0; 1 -1 0], # chiral_shape_2\n",
    "          [0 0 0; 1 0 0; 0 1 0; 1 1 0],  # square\n",
    "          [0 0 0; 0 0 1; 0 0 2; 0 0 3],  # line\n",
    "          [0 0 0; 0 0 1; 0 1 0; 1 0 0],  # corner\n",
    "          [0 0 0; 0 0 1; 0 0 2; 0 1 0],  # T\n",
    "          [0 0 0; 0 0 1; 0 0 2; 0 1 1],  # zigzag\n",
    "          [0 0 0; 1 0 0; 1 1 0; 2 1 0]]  # L\n",
    "\n",
    "onehot_tetris = onehotbatch(1:length(tetris) |> collect, 1:length(tetris))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"TFNLayers.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RLayer{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Vector{Float32}, Float32}(Float32[-0.48342064, -0.5924541, -0.46601388, -0.4184361], Float32[0.0, 1.1666666, 2.3333333, 3.5], 0.875f0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing R\n",
    "centers = range(0f0, 3.5f0; length=4) |> collect\n",
    "r_test = RLayer(centers) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}[], CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}[[-0.36121196 -0.021606624 … 0.20287748 0.41855606; 0.036829777 0.3516178 … -0.5751697 -0.262716; 0.041203 0.24422988 … -0.07556881 -0.031145327; 0.08022238 -0.7771977 … 0.24490415 -0.32765147;;; -0.34861088 0.035238333 … 0.08414009 0.3777876; 0.06861416 0.43490127 … -0.46269113 -0.11485022; -0.07405516 0.116280064 … -0.15479514 -0.1404848; 0.119697616 -0.8207739 … 0.29899177 -0.35680676;;; -0.3369673 0.08685716 … 0.04711111 0.26186392; 0.09838154 0.47218075 … -0.26507103 0.06836632; -0.2230749 -0.029416293 … -0.2969591 -0.24748111; 0.25870395 -0.7325784 … 0.31196216 -0.28570586]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "n_points = 4\n",
    "ℓi, ℓf, ℓos = 2, 1, [1]\n",
    "total_outs = [2ℓo + 1 for ℓo in ℓos]\n",
    "\n",
    "centers = range(0f0, 3.5f0; length=4) |> collect\n",
    "c_test = CLayer((ℓi, ℓf) => ℓos, centers) |> gpu\n",
    "\n",
    "xs = rand(Float32, (n_points, 3, n_samples))\n",
    "xss = pairwise_rs(xs)\n",
    "rss = cart_to_sph(xss) |> gpu\n",
    "\n",
    "V = ones(Float32, (n_points, n_samples, 2ℓi+1)) |> gpu\n",
    "#V = ones(Float32, (1, 4))\n",
    "outs_fake = rand(Float32, (n_points, n_samples, total_outs[1])) |> gpu\n",
    "\n",
    "C_out = c_test(rss, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:43\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "optim = Flux.setup(Flux.Adam(0.1), c_test)\n",
    "\n",
    "losses = []\n",
    "@showprogress for epoch in 1:400\n",
    "    loss, grads = Flux.withgradient(c_test) do f\n",
    "        # Evaluate model and loss inside gradient context:\n",
    "        y_hat = f(rss, V)\n",
    "        Flux.mse(y_hat[2][1], outs_fake)\n",
    "    end\n",
    "    Flux.update!(optim, c_test, grads[1])\n",
    "    push!(losses, loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:01\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "a = (V, ([V], [V, V], []))\n",
    "\n",
    "p_test = Chain(\n",
    "            # Self interaction\n",
    "            Parallel(triv_connect, identity, Parallel(triv_connect, SILayer(1 => 2), SILayer(2 => 3), identity)),\n",
    "            # Nonlinearity\n",
    "            Parallel(triv_connect, NLLayer(2), NLLayer(3), identity)\n",
    " ) |> gpu\n",
    "\n",
    "#ou = p_test(a)[1][1]\n",
    "optim = Flux.setup(Flux.Adam(0.1), p_test)\n",
    "\n",
    "losses = []\n",
    "    @showprogress for epoch in 1:400\n",
    "    loss, grads = Flux.withgradient(p_test) do f\n",
    "        # Evaluate model and loss inside gradient context:\n",
    "        y_hat = f(a)[1][1]\n",
    "        Flux.mse(y_hat, V)\n",
    "    end;\n",
    "    Flux.update!(optim, p_test, grads[1])\n",
    "    push!(losses, loss)\n",
    "end\n",
    "\n",
    "\n",
    "#nl_test = NLLayer(1) |> gpu\n",
    "#nl_test([V]) |> typeof\n",
    "\n",
    "#combined = Tuple(vcat(x, y) for (x, y) in zip($a, $b))\n",
    "#reduce((x, y) -> (x..., y...), (a, b, c)) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0], (CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}[[0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787;;; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787;;; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787;;; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787;;; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787; 0.58756787 0.58756787 … 0.58756787 0.58756787], [-0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583;;; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583;;; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583;;; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583;;; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583; -0.5641583 -0.5641583 … -0.5641583 -0.5641583]], CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}[[-0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992;;; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992;;; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992;;; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992;;; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992; -0.14826992 -0.14826992 … -0.14826992 -0.14826992], [1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193;;; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193;;; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193;;; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193;;; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193; 1.0799193 1.0799193 … 1.0799193 1.0799193], [-0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764;;; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764;;; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764;;; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764;;; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764; -0.63686764 -0.63686764 … -0.63686764 -0.63686764]], Any[]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (V, ([V], [V, V], []))\n",
    "\n",
    "p_passenger = Chain(\n",
    "            # Self interaction\n",
    "            Parallel(triv_connect, identity, Parallel(triv_connect, SILayer(1 => 2), SILayer(2 => 3), identity))\n",
    ") |> gpu\n",
    "\n",
    "\n",
    "p_passenger(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vector{eltype([V])}(undef, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{CuArray{Float32, 3, CUDA.Mem.DeviceBuffer}}:\n",
       " [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]\n",
       " [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]\n",
       " [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]\n",
       " [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ([V], [V, V], Vector{typeof(V)}(undef, 0))\n",
    "b = (Vector{typeof(V)}(undef, 0), [V], [V])\n",
    "c = ([V], [V], [V])\n",
    "\n",
    "#@btime Tuple(vcat(x, y) for (x, y) in zip($a, $b)) # Pretty fast\n",
    "\n",
    "function tuple_connect(xa, ya)\n",
    "    Tuple(vcat(x, y) for (x, y) in zip(xa, ya))\n",
    "end\n",
    "\n",
    "answer = reduce(tuple_connect, (a, b, c))\n",
    "\n",
    "answer[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×6 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       "  0.298705   0.298705   0.298705  -0.331017   -0.331017   -0.331017\n",
       "  0.192269   0.192269   0.192269  -0.0233604  -0.0233604  -0.0233604\n",
       " -0.479108  -0.479108  -0.479108  -0.0682962  -0.0682962  -0.0682962\n",
       "  1.02654    1.02654    1.02654    0.583121    0.583121    0.583121"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Parallel(hcat, Dense(1 => 4), Dense(1 => 4)) |> gpu\n",
    "d_test = Dense(1 => 4) |> gpu\n",
    "\n",
    "p_compat = (x -> p(x...))\n",
    "\n",
    "test_mat = ones(Float32, (1, 3)) |> gpu\n",
    "\n",
    "c_test = Chain(p_compat, Dense(4 => 1)) |> gpu\n",
    "\n",
    "p([test_mat, test_mat]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parallel(\n",
       "  tuple_connect,\n",
       "  Chain(\n",
       "    var\"#534#536\"(),\n",
       "    Parallel(\n",
       "      tuple_connect,\n",
       "      CLayer(\n",
       "        FLayer(\n",
       "          RLayer{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Vector{Float32}, Float32}(Float32[-0.6948973, 0.8630991, -0.07906914, -0.94141114], Float32[0.0, 1.1666666, 2.3333333, 3.5], 0.875f0),  \u001b[90m# 4 parameters\u001b[39m\n",
       "        ),\n",
       "      ),\n",
       "      CLayer(\n",
       "        FLayer(\n",
       "          RLayer{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Vector{Float32}, Float32}(Float32[-0.38486096, 0.39335984, 0.77282935, -0.76927626], Float32[0.0, 1.1666666, 2.3333333, 3.5], 0.875f0),  \u001b[90m# 4 parameters\u001b[39m\n",
       "        ),\n",
       "      ),\n",
       "    ),\n",
       "  ),\n",
       ") "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ℓi = 0\n",
    "ℓ_max = 1\n",
    "in_channel = []\n",
    "n_c = 2\n",
    "for (ℓf, ℓos) in [1 => [1]]\n",
    "    channel_convs = Tuple(CLayer((ℓi, ℓf) => ℓos, centers; ℓ_max = ℓ_max) for c in 1:n_c)\n",
    "\n",
    "    p = Parallel(tuple_connect, channel_convs)\n",
    "    push!(in_channel, Chain(x -> Tuple(x), p))\n",
    "    #push!(in_channel, p)\n",
    "end\n",
    "\n",
    "#in_channels[1]\n",
    "p_out = Parallel(tuple_connect, in_channel...) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18.337 ns (1 allocation: 64 bytes)\n",
      "  79.959 ns (3 allocations: 64 bytes)\n",
      "  48.077 ns (1 allocation: 32 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0 0.43549958 1.0521978 0.56605774; 0.43549958 0.0 1.3980144 0.9712511; 1.0521978 1.3980144 0.0 0.8892459; 0.56605774 0.9712511 0.8892459 0.0;;; 0.0 2.3328195 1.0250881 0.37029645; 0.8087733 0.0 0.92014986 0.5493338; 2.1165047 2.2214427 0.0 1.5915178; 2.7712963 2.592259 1.5500748 0.0;;; 0.0 1.4255625 -0.7652147 -2.1712863; -1.7160301 0.0 -0.99782175 -1.8945957; 2.376378 2.143771 0.0 2.605678; 0.97030634 1.2469969 -0.53591454 0.0;;;; 0.0 0.7547935 0.6804147 0.20409416; 0.7547935 0.0 0.28264362 0.59508324; 0.6804147 0.28264362 0.0 0.5821993; 0.20409416 0.59508324 0.5821993 0.0;;; 0.0 1.5146486 1.5841296 1.1584036; 1.6269442 0.0 1.753775 1.504465; 1.557463 1.3878177 0.0 1.4140692; 1.9831891 1.6371276 1.7275236 0.0;;; 0.0 0.7300529 0.35341373 1.2074509; -2.4115398 0.0 -1.2909464 -2.5567398; -2.788179 1.8506463 0.0 -3.0358489; -1.9341418 0.584853 0.10574378 0.0;;;; 0.0 0.23809826 0.75272274 0.4063107; 0.23809826 0.0 0.8707703 0.52639395; 0.75272274 0.8707703 0.0 0.3507919; 0.4063107 0.52639395 0.3507919 0.0;;; 0.0 0.9950294 2.9156675 2.8391087; 2.1465633 0.0 3.0099263 2.9575412; 0.22592524 0.13166627 0.0 0.17008683; 0.3024839 0.18405135 2.9715059 0.0;;; 0.0 1.079581 1.6884321 1.4391255; -2.0620117 0.0 -3.065888 -2.5199015; -1.4531605 0.07570469 0.0 -0.9261585; -1.7024671 0.62169117 2.215434 0.0;;;; … ;;;; 0.0 0.46838564 0.5730394 0.44515583; 0.46838564 0.0 0.2774533 0.49560633; 0.5730394 0.2774533 0.0 0.5287956; 0.44515583 0.49560633 0.5287956 0.0;;; 0.0 1.2909136 0.85142636 1.5985639; 1.850679 0.0 0.46339047 1.8608549; 2.2901664 2.6782022 0.0 2.40003; 1.5430287 1.2807378 0.7415628 0.0;;; 0.0 -1.2235575 -0.94447887 -0.105152465; 1.9180351 0.0 0.6399502 0.9156795; 2.1971138 -2.5016425 0.0 1.0103514; 3.0364401 -2.225913 -2.1312413 0.0;;;; 0.0 0.34015885 1.0369872 0.72072834; 0.34015885 0.0 0.8899549 0.9455485; 1.0369872 0.8899549 0.0 0.9986987; 0.72072834 0.9455485 0.9986987 0.0;;; 0.0 1.42084 0.45260394 0.7071389; 1.7207527 0.0 0.13584134 1.0172312; 2.6889887 3.0057514 0.0 1.9661784; 2.4344537 2.1243615 1.1754142 0.0;;; 0.0 0.17541066 0.2479905 -2.9172206; -2.966182 0.0 0.45177037 -2.9376874; -2.8936021 -2.6898222 0.0 -2.9056003; 0.22437216 0.20390517 0.23599239 0.0;;;; 0.0 0.68959695 0.83092 0.5034629; 0.68959695 0.0 0.96320695 0.5179265; 0.83092 0.96320695 0.0 0.61538273; 0.5034629 0.5179265 0.61538273 0.0;;; 0.0 2.3549669 1.5476671 1.5956312; 0.78662586 0.0 1.0174068 0.41233674; 1.5939255 2.124186 0.0 1.6223626; 1.5459614 2.729256 1.51923 0.0;;; 0.0 1.4571805 2.7056763 1.8778843; -1.6844121 0.0 -2.9769409 -3.1160192; -0.4359163 0.16465177 0.0 0.21148917; -1.2637084 0.025573429 -2.9301035 0.0], [1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0;;; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_vec = [rss]\n",
    "V_vec = [V]\n",
    "\n",
    "rss_tup = (rss,)\n",
    "V_tup = (V,)\n",
    "\n",
    "@btime vcat($rss_vec, $V_vec)\n",
    "@btime (rss_tup..., V_tup)\n",
    "@btime (rss_vec..., V_vec...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Increasing Dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4×6×5 CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}:\n",
       "[:, :, 1, 1] =\n",
       " -0.0          0.00754981  -0.0499345  -0.116065\n",
       "  0.00754981  -0.0         -0.213974   -0.329704\n",
       " -0.0499345   -0.213974    -0.0         0.545569\n",
       " -0.116065    -0.329704     0.545569   -0.0\n",
       "\n",
       "[:, :, 2, 1] =\n",
       " -0.0        0.232802   0.160161    0.262934\n",
       "  0.232802  -0.0       -0.229806    0.456423\n",
       "  0.160161  -0.229806  -0.0         0.00174865\n",
       "  0.262934   0.456423   0.0017487  -0.0\n",
       "\n",
       "[:, :, 3, 1] =\n",
       " -0.0        -0.370857   0.226216    -0.0809055\n",
       " -0.370857   -0.0       -0.503037    -0.22966\n",
       "  0.226216   -0.503037  -0.0          0.00735487\n",
       " -0.0809054  -0.22966    0.00735485  -0.0\n",
       "\n",
       "[:, :, 4, 1] =\n",
       " -0.0       -0.11373     0.145963   0.258541\n",
       " -0.11373   -0.0         0.113866  -0.0867647\n",
       "  0.145963   0.113866   -0.0        0.141403\n",
       "  0.258541  -0.0867647   0.141403  -0.0\n",
       "\n",
       "[:, :, 5, 1] =\n",
       " -0.0        0.166011  -0.113239   0.15408\n",
       "  0.166011  -0.0        0.425438  -0.265191\n",
       " -0.113239   0.425438  -0.0        0.450372\n",
       "  0.15408   -0.265191   0.450372  -0.0\n",
       "\n",
       "[:, :, 6, 1] =\n",
       " -0.0        0.130746   -0.553958   -0.55348\n",
       "  0.130746  -0.0        -0.0236442   0.0446081\n",
       " -0.553958  -0.0236441  -0.0         0.104389\n",
       " -0.55348    0.0446081   0.104389   -0.0\n",
       "\n",
       "[:, :, 1, 2] =\n",
       "  0.0         0.0440398  -0.0989317  -0.343865\n",
       "  0.0440399   0.0        -0.205881   -0.484191\n",
       " -0.0989317  -0.205881    0.0         0.239383\n",
       " -0.343865   -0.484191    0.239383    0.0\n",
       "\n",
       "[:, :, 2, 2] =\n",
       "  0.0       -0.409888  -0.335243     -0.30574\n",
       " -0.409888   0.0        0.23382      -0.17016\n",
       " -0.335243   0.23382    0.0          -0.000978819\n",
       " -0.30574   -0.17016   -0.000978845   0.0\n",
       "\n",
       "[:, :, 3, 2] =\n",
       "  0.0        0.14923   -0.468468    -0.182065\n",
       "  0.14923    0.0       -0.202335    -0.174967\n",
       " -0.468468  -0.202335   0.0          0.00721453\n",
       " -0.182065  -0.174967   0.00721451   0.0\n",
       "\n",
       "[:, :, 4, 2] =\n",
       "  0.0        0.463575  -0.416647  -0.423038\n",
       "  0.463575   0.0       -0.341982   0.215508\n",
       " -0.416647  -0.341982   0.0       -0.464406\n",
       " -0.423038   0.215508  -0.464406   0.0\n",
       "\n",
       "[:, :, 5, 2] =\n",
       "  0.0       -0.177267   0.529402  -0.137167\n",
       " -0.177267   0.0       -0.224481  -0.502218\n",
       "  0.529402  -0.224481   0.0       -0.163129\n",
       " -0.137167  -0.502218  -0.163129   0.0\n",
       "\n",
       "[:, :, 6, 2] =\n",
       "  0.0       -0.3974     0.189484    0.164832\n",
       " -0.3974     0.0        0.0340704  -0.0622461\n",
       "  0.189484   0.0340704  0.0         0.124179\n",
       "  0.164831  -0.0622461  0.124179    0.0\n",
       "\n",
       "[:, :, 1, 3] =\n",
       " -0.711469  -0.643691   -0.39365   -0.388167\n",
       " -0.643691  -0.711469   -0.117377   0.0452996\n",
       " -0.39365   -0.117378   -0.711469   0.287742\n",
       " -0.388167   0.0452995   0.287742  -0.711469\n",
       "\n",
       "[:, :, 2, 3] =\n",
       " -0.711469   -0.139168   0.106601    0.0456502\n",
       " -0.139168   -0.711469   0.287216    0.296927\n",
       "  0.1066      0.287216  -0.711469    0.0970911\n",
       "  0.0456503   0.296927   0.0970912  -0.711469\n",
       "\n",
       "[:, :, 3, 3] =\n",
       " -0.711469   0.207179   -0.262908  -0.36682\n",
       "  0.207179  -0.711469    0.268988   0.0121537\n",
       " -0.262908   0.268988   -0.711469  -0.138683\n",
       " -0.36682    0.0121537  -0.138683  -0.711469\n",
       "\n",
       "[:, :, 4, 3] =\n",
       " -0.711469    0.156149  -0.0915902   0.191237\n",
       "  0.156149   -0.711469  -0.3052     -0.522433\n",
       " -0.0915902  -0.3052    -0.711469   -0.200015\n",
       "  0.191237   -0.522433  -0.200015   -0.711469\n",
       "\n",
       "[:, :, 5, 3] =\n",
       " -0.711469   -0.123787  -0.294562  -0.0683236\n",
       " -0.123786   -0.711469   0.177467  -0.229151\n",
       " -0.294562    0.177467  -0.711469   0.22338\n",
       " -0.0683236  -0.229151   0.22338   -0.711469\n",
       "\n",
       "[:, :, 6, 3] =\n",
       " -0.711469  -0.375584   0.291724   0.292187\n",
       " -0.375584  -0.711469  -0.29742   -0.282837\n",
       "  0.291724  -0.29742   -0.711469   0.34662\n",
       "  0.292187  -0.282837   0.34662   -0.711469\n",
       "\n",
       "[:, :, 1, 4] =\n",
       "  0.0       -0.194129  -0.397966  -0.267353\n",
       " -0.194129   0.0       -0.549932  -0.233488\n",
       " -0.397966  -0.549932   0.0       -0.173205\n",
       " -0.267353  -0.233488  -0.173205   0.0\n",
       "\n",
       "[:, :, 2, 4] =\n",
       " 0.0        0.279028   0.0804314  0.186386\n",
       " 0.279028   0.0        0.0512307  0.09115\n",
       " 0.0804314  0.0512307  0.0        0.504255\n",
       " 0.186386   0.09115    0.504255   0.0\n",
       "\n",
       "[:, :, 3, 4] =\n",
       "  0.0        0.326114   0.335294  -0.324459\n",
       "  0.326114   0.0       -0.14965   -0.347819\n",
       "  0.335294  -0.14965    0.0       -0.509893\n",
       " -0.324459  -0.347819  -0.509893   0.0\n",
       "\n",
       "[:, :, 4, 4] =\n",
       " 0.0        0.0543267  0.140173  0.11342\n",
       " 0.0543267  0.0        0.217843  0.406516\n",
       " 0.140173   0.217843   0.0       0.171676\n",
       " 0.11342    0.406516   0.171676  0.0\n",
       "\n",
       "[:, :, 5, 4] =\n",
       " 0.0        0.416529  0.154231   0.434218\n",
       " 0.416529   0.0       0.223743  -0.357746\n",
       " 0.154231   0.223743  0.0        0.194471\n",
       " 0.434218  -0.357746  0.194471   0.0\n",
       "\n",
       "[:, :, 6, 4] =\n",
       " 0.0       0.260907   0.162092    0.20645\n",
       " 0.260907  0.0        0.471674    0.47519\n",
       " 0.162092  0.471674   0.0        -0.0106505\n",
       " 0.20645   0.47519   -0.0106505   0.0\n",
       "\n",
       "[:, :, 1, 5] =\n",
       " -0.0        -0.0157836  -0.0942272   0.0295205\n",
       " -0.0157836  -0.0        -0.245721    0.262363\n",
       " -0.0942272  -0.245721   -0.0         0.179635\n",
       "  0.0295205   0.262363    0.179635   -0.0\n",
       "\n",
       "[:, :, 2, 5] =\n",
       " -0.0         0.0917521   0.314568   0.135509\n",
       "  0.0917521  -0.0         0.499248   0.303782\n",
       "  0.314568    0.499248   -0.0       -0.450423\n",
       "  0.135509    0.303782   -0.450423  -0.0\n",
       "\n",
       "[:, :, 3, 5] =\n",
       " -0.0        -0.320366   0.0770783  -0.0493918\n",
       " -0.320366   -0.0        0.154041   -0.170508\n",
       "  0.0770783   0.154041  -0.0        -0.259854\n",
       " -0.0493918  -0.170508  -0.259854   -0.0\n",
       "\n",
       "[:, :, 4, 5] =\n",
       " -0.0        0.478571    0.192375    0.447498\n",
       "  0.478571  -0.0         0.0531103  -0.0588345\n",
       "  0.192375   0.0531103  -0.0         0.165121\n",
       "  0.447498  -0.0588345   0.165121   -0.0\n",
       "\n",
       "[:, :, 5, 5] =\n",
       " -0.0       -0.159715     0.177854    -0.219542\n",
       " -0.159715  -0.0          0.00140014   0.0916911\n",
       "  0.177854   0.00140017  -0.0         -0.0795562\n",
       " -0.219542   0.0916911   -0.0795561   -0.0\n",
       "\n",
       "[:, :, 6, 5] =\n",
       " -0.0         0.0566532   0.0868458  -0.125663\n",
       "  0.0566532  -0.0        -0.162812   -0.167349\n",
       "  0.0868456  -0.162812   -0.0         0.60408\n",
       " -0.125663   -0.167349    0.60408    -0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 6\n",
    "n_points = 4\n",
    "\n",
    "xs = rand(Float32, (n_points, 3, n_samples))\n",
    "xss = pairwise_rs(xs)\n",
    "rss = cart_to_sph(xss)\n",
    "\n",
    "ℓ = 2\n",
    "Ys = generate_Yℓms(ℓ)\n",
    "yss = rand(Float32, (n_points, n_points, n_samples, 2ℓ+1))\n",
    "\n",
    "rss_gpu = rss |> gpu\n",
    "yss_gpu = yss |> gpu\n",
    "\n",
    "centers = range(0f0, 3.5f0; length=4) |> collect\n",
    "f_test = FLayer(Ys, centers) |> gpu\n",
    "\n",
    "f_test(rss_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:36\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "optim = Flux.setup(Flux.Adam(0.01), f_test)\n",
    "\n",
    "# Testing gradient\n",
    "losses = []\n",
    "@showprogress for epoch in 1:400\n",
    "    loss, grads = Flux.withgradient(f_test) do f\n",
    "        # Evaluate model and loss inside gradient context:\n",
    "        y_hat = f(rss_gpu)\n",
    "        Flux.mse(y_hat, yss_gpu)\n",
    "    end\n",
    "    Flux.update!(optim, f_test, grads[1])\n",
    "    push!(losses, loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
